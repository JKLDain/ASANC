{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 noisy_dataset,\n",
    "                 limit,\n",
    "                 offset,\n",
    "                 sr,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            noisy_dataset (str): noisy dir (wav format files) or noisy filenames list\n",
    "        \"\"\"\n",
    "        noisy_dataset = os.path.abspath(os.path.expanduser(noisy_dataset))\n",
    "\n",
    "        if os.path.isfile(noisy_dataset):\n",
    "            noisy_wav_files = [line.rstrip('\\n') for line in open(os.path.abspath(os.path.expanduser(noisy_dataset)), \"r\")]\n",
    "            if offset:\n",
    "                noisy_wav_files = noisy_wav_files[offset:]\n",
    "            if limit:\n",
    "                noisy_wav_files = noisy_wav_files[:limit]\n",
    "        elif os.path.isdir(noisy_dataset):\n",
    "            noisy_wav_files = librosa.util.find_files(noisy_dataset, ext=\"wav\", limit=limit, offset=offset)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Please Check {noisy_dataset}\")\n",
    "\n",
    "        print(f\"Number of noisy files in the dir {noisy_dataset}: {len(noisy_wav_files)}\")\n",
    "\n",
    "        self.length = len(noisy_wav_files)\n",
    "        self.noisy_wav_files = noisy_wav_files\n",
    "        self.sr = sr\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        noisy_path = self.noisy_wav_files[item]\n",
    "        name = os.path.splitext(os.path.basename(noisy_path))[0]\n",
    "        noisy, _ = librosa.load(noisy_path, sr=self.sr)\n",
    "        return noisy, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_list,\n",
    "            limit,\n",
    "            offset,\n",
    "            sr,\n",
    "            n_fft,\n",
    "            hop_length,\n",
    "            train\n",
    "    ):\n",
    "        \"\"\"\n",
    "        dataset_list(*.txt):\n",
    "            <noisy_path> <clean_path>\\n\n",
    "        e.g:\n",
    "            noisy_1.wav clean_1.wav\n",
    "            noisy_2.wav clean_2.wav\n",
    "            ...\n",
    "            noisy_n.wav clean_n.wav\n",
    "        \"\"\"\n",
    "        super(Dataset, self).__init__()\n",
    "        self.sr = sr\n",
    "        self.train = train\n",
    "\n",
    "        dataset_list = [line.rstrip('\\n') for line in open(os.path.abspath(os.path.expanduser(dataset_list)), \"r\")]\n",
    "        dataset_list = dataset_list[offset:]\n",
    "        if limit:\n",
    "            dataset_list = dataset_list[:limit]\n",
    "\n",
    "        self.dataset_list = dataset_list\n",
    "        self.length = len(self.dataset_list)\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        noisy_path, clean_path = self.dataset_list[item].split(\" \")\n",
    "        name = os.path.splitext(os.path.basename(noisy_path))[0]\n",
    "        noisy, _ = librosa.load(os.path.abspath(os.path.expanduser(noisy_path)), sr=self.sr)\n",
    "        clean, _ = librosa.load(os.path.abspath(os.path.expanduser(clean_path)), sr=self.sr)\n",
    "\n",
    "        if self.train:\n",
    "            noisy_mag, _ = librosa.magphase(librosa.stft(noisy, n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.n_fft))\n",
    "            clean_mag, _ = librosa.magphase(librosa.stft(clean, n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.n_fft))\n",
    "            return noisy_mag, clean_mag, noisy_mag.shape[-1], name\n",
    "        else:\n",
    "            return noisy, clean, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class WavDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Define train dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mixture_dataset,\n",
    "                 clean_dataset,\n",
    "                 limit=None,\n",
    "                 offset=0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Construct train dataset\n",
    "        Args:\n",
    "            mixture_dataset (str): mixture dir (wav format files)\n",
    "            clean_dataset (str): clean dir (wav format files)\n",
    "            limit (int): the limit of the dataset\n",
    "            offset (int): the offset of the dataset\n",
    "        \"\"\"\n",
    "        assert os.path.exists(mixture_dataset) and os.path.exists(clean_dataset)\n",
    "\n",
    "        print(\"Search datasets...\")\n",
    "        mixture_wav_files = librosa.util.find_files(mixture_dataset, ext=\"wav\", limit=limit, offset=offset)\n",
    "        clean_wav_files = librosa.util.find_files(clean_dataset, ext=\"wav\", limit=limit, offset=offset)\n",
    "\n",
    "        assert len(mixture_wav_files) == len(clean_wav_files)\n",
    "        print(f\"\\t Original length: {len(mixture_wav_files)}\")\n",
    "\n",
    "        self.length = len(mixture_wav_files)\n",
    "        self.mixture_wav_files = mixture_wav_files\n",
    "        self.clean_wav_files = clean_wav_files\n",
    "\n",
    "        print(f\"\\t Offset: {offset}\")\n",
    "        print(f\"\\t Limit: {limit}\")\n",
    "        print(f\"\\t Final length: {self.length}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        mixture_path = self.mixture_wav_files[item]\n",
    "        clean_path = self.clean_wav_files[item]\n",
    "        name = os.path.splitext(os.path.basename(clean_path))[0]\n",
    "\n",
    "        mixture, sr = sf.read(mixture_path, dtype=\"float32\")\n",
    "        clean, sr = sf.read(clean_path, dtype=\"float32\")\n",
    "        assert sr == 16000\n",
    "        assert mixture.shape == clean.shape\n",
    "\n",
    "        n_frames = (len(mixture) - 320) // 160 + 1\n",
    "\n",
    "        return mixture, clean, n_frames, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class WavDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Define train dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mixture_dataset,\n",
    "                 clean_dataset,\n",
    "                 limit=None,\n",
    "                 offset=0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Construct train dataset\n",
    "        Args:\n",
    "            mixture_dataset (str): mixture dir (wav format files)\n",
    "            clean_dataset (str): clean dir (wav format files)\n",
    "            limit (int): the limit of the dataset\n",
    "            offset (int): the offset of the dataset\n",
    "        \"\"\"\n",
    "        mixture_dataset = os.path.abspath(os.path.expanduser(mixture_dataset))\n",
    "        clean_dataset = os.path.abspath(os.path.expanduser(clean_dataset))\n",
    "        assert os.path.exists(mixture_dataset) and os.path.exists(clean_dataset)\n",
    "\n",
    "        print(\"Search datasets...\")\n",
    "        mixture_wav_files = librosa.util.find_files(mixture_dataset, ext=\"wav\", limit=limit, offset=offset)\n",
    "        clean_wav_files = librosa.util.find_files(clean_dataset, ext=\"wav\", limit=limit, offset=offset)\n",
    "\n",
    "        assert len(mixture_wav_files) == len(clean_wav_files)\n",
    "        print(f\"\\t Original length: {len(mixture_wav_files)}\")\n",
    "\n",
    "        self.length = len(mixture_wav_files)\n",
    "        self.mixture_wav_files = mixture_wav_files\n",
    "        self.clean_wav_files = clean_wav_files\n",
    "\n",
    "        print(f\"\\t Offset: {offset}\")\n",
    "        print(f\"\\t Limit: {limit}\")\n",
    "        print(f\"\\t Final length: {self.length}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        mixture_path = self.mixture_wav_files[item]\n",
    "        clean_path = self.clean_wav_files[item]\n",
    "        name = os.path.splitext(os.path.basename(clean_path))[0]\n",
    "\n",
    "        mixture, sr = sf.read(mixture_path, dtype=\"float32\")\n",
    "        clean, sr = sf.read(clean_path, dtype=\"float32\")\n",
    "        assert sr == 16000\n",
    "        assert mixture.shape == clean.shape\n",
    "\n",
    "        n_frames = (len(mixture) - 320) // 160 + 1\n",
    "\n",
    "        return mixture, clean, n_frames, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 161, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CausalConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 2),\n",
    "            stride=(2, 1),\n",
    "            padding=(0, 1)\n",
    "        )\n",
    "        self.norm = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.activation = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        2D Causal convolution.\n",
    "        Args:\n",
    "            x: [B, C, F, T]\n",
    "        Returns:\n",
    "            [B, C, F, T]\n",
    "        \"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = x[:, :, :, :-1]  # chomp size\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalTransConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, is_last=False, output_padding=(0, 0)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 2),\n",
    "            stride=(2, 1),\n",
    "            output_padding=output_padding\n",
    "        )\n",
    "        self.norm = nn.BatchNorm2d(num_features=out_channels)\n",
    "        if is_last:\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            self.activation = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        2D Causal convolution.\n",
    "        Args:\n",
    "            x: [B, C, F, T]\n",
    "        Returns:\n",
    "            [B, C, F, T]\n",
    "        \"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = x[:, :, :, :-1]  # chomp size\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CRN(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: [batch size, channels=1, T, n_fft]\n",
    "    Output: [batch size, T, n_fft]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CRN, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv_block_1 = CausalConvBlock(1, 16)\n",
    "        self.conv_block_2 = CausalConvBlock(16, 32)\n",
    "        self.conv_block_3 = CausalConvBlock(32, 64)\n",
    "        self.conv_block_4 = CausalConvBlock(64, 128)\n",
    "        self.conv_block_5 = CausalConvBlock(128, 256)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm_layer = nn.LSTM(input_size=1024, hidden_size=1024, num_layers=2, batch_first=True)\n",
    "\n",
    "        self.tran_conv_block_1 = CausalTransConvBlock(256 + 256, 128)\n",
    "        self.tran_conv_block_2 = CausalTransConvBlock(128 + 128, 64)\n",
    "        self.tran_conv_block_3 = CausalTransConvBlock(64 + 64, 32)\n",
    "        self.tran_conv_block_4 = CausalTransConvBlock(32 + 32, 16, output_padding=(1, 0))\n",
    "        self.tran_conv_block_5 = CausalTransConvBlock(16 + 16, 1, is_last=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm_layer.flatten_parameters()\n",
    "\n",
    "        e_1 = self.conv_block_1(x)\n",
    "        e_2 = self.conv_block_2(e_1)\n",
    "        e_3 = self.conv_block_3(e_2)\n",
    "        e_4 = self.conv_block_4(e_3)\n",
    "        e_5 = self.conv_block_5(e_4)  # [2, 256, 4, 200]\n",
    "\n",
    "        batch_size, n_channels, n_f_bins, n_frame_size = e_5.shape\n",
    "\n",
    "        # [2, 256, 4, 200] = [2, 1024, 200] => [2, 200, 1024]\n",
    "        lstm_in = e_5.reshape(batch_size, n_channels * n_f_bins, n_frame_size).permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm_layer(lstm_in)  # [2, 200, 1024]\n",
    "        lstm_out = lstm_out.permute(0, 2, 1).reshape(batch_size, n_channels, n_f_bins, n_frame_size)  # [2, 256, 4, 200]\n",
    "\n",
    "        d_1 = self.tran_conv_block_1(torch.cat((lstm_out, e_5), 1))\n",
    "        d_2 = self.tran_conv_block_2(torch.cat((d_1, e_4), 1))\n",
    "        d_3 = self.tran_conv_block_3(torch.cat((d_2, e_3), 1))\n",
    "        d_4 = self.tran_conv_block_4(torch.cat((d_3, e_2), 1))\n",
    "        d_5 = self.tran_conv_block_5(torch.cat((d_4, e_1), 1))\n",
    "\n",
    "        return d_5\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    layer = CRN()\n",
    "    a = torch.rand(2, 1, 161, 200)\n",
    "    print(layer(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "TensorBoard logging requires TensorBoard version 1.15 or above",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7ad23990de40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush_secs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__version__'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1.15'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TensorBoard logging requires TensorBoard version 1.15 or above'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: TensorBoard logging requires TensorBoard version 1.15 or above"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def writer(logs_dir):\n",
    "    return SummaryWriter(log_dir=logs_dir, max_queue=5, flush_secs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dainjeong77\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\dainjeong77\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\dainjeong77\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\dainjeong77\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\dainjeong77\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\dainjeong77\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "tf.__version__\n",
    "\n",
    "import tensorflow as tf\n",
    "print (tf.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
