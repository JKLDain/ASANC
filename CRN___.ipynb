{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 noisy_dataset,\n",
    "                 limit,\n",
    "                 offset,\n",
    "                 sr,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            noisy_dataset (str): noisy dir (wav format files) or noisy filenames list\n",
    "        \"\"\"\n",
    "        noisy_dataset = os.path.abspath(os.path.expanduser(noisy_dataset))\n",
    "\n",
    "        if os.path.isfile(noisy_dataset):\n",
    "            noisy_wav_files = [line.rstrip('\\n') for line in open(os.path.abspath(os.path.expanduser(noisy_dataset)), \"r\")]\n",
    "            if offset:\n",
    "                noisy_wav_files = noisy_wav_files[offset:]\n",
    "            if limit:\n",
    "                noisy_wav_files = noisy_wav_files[:limit]\n",
    "        elif os.path.isdir(noisy_dataset):\n",
    "            noisy_wav_files = librosa.util.find_files(noisy_dataset, ext=\"wav\", limit=limit, offset=offset)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Please Check {noisy_dataset}\")\n",
    "\n",
    "        print(f\"Number of noisy files in the dir {noisy_dataset}: {len(noisy_wav_files)}\")\n",
    "\n",
    "        self.length = len(noisy_wav_files)\n",
    "        self.noisy_wav_files = noisy_wav_files\n",
    "        self.sr = sr\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        noisy_path = self.noisy_wav_files[item]\n",
    "        name = os.path.splitext(os.path.basename(noisy_path))[0]\n",
    "        noisy, _ = librosa.load(noisy_path, sr=self.sr)\n",
    "        return noisy, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_list,\n",
    "            limit,\n",
    "            offset,\n",
    "            sr,\n",
    "            n_fft,\n",
    "            hop_length,\n",
    "            train\n",
    "    ):\n",
    "        \"\"\"\n",
    "        dataset_list(*.txt):\n",
    "            <noisy_path> <clean_path>\\n\n",
    "        e.g:\n",
    "            noisy_1.wav clean_1.wav\n",
    "            noisy_2.wav clean_2.wav\n",
    "            ...\n",
    "            noisy_n.wav clean_n.wav\n",
    "        \"\"\"\n",
    "        super(Dataset, self).__init__()\n",
    "        self.sr = sr\n",
    "        self.train = train\n",
    "\n",
    "        dataset_list = [line.rstrip('\\n') for line in open(os.path.abspath(os.path.expanduser(dataset_list)), \"r\")]\n",
    "        dataset_list = dataset_list[offset:]\n",
    "        if limit:\n",
    "            dataset_list = dataset_list[:limit]\n",
    "\n",
    "        self.dataset_list = dataset_list\n",
    "        self.length = len(self.dataset_list)\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        noisy_path, clean_path = self.dataset_list[item].split(\" \")\n",
    "        name = os.path.splitext(os.path.basename(noisy_path))[0]\n",
    "        noisy, _ = librosa.load(os.path.abspath(os.path.expanduser(noisy_path)), sr=self.sr)\n",
    "        clean, _ = librosa.load(os.path.abspath(os.path.expanduser(clean_path)), sr=self.sr)\n",
    "\n",
    "        if self.train:\n",
    "            noisy_mag, _ = librosa.magphase(librosa.stft(noisy, n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.n_fft))\n",
    "            clean_mag, _ = librosa.magphase(librosa.stft(clean, n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.n_fft))\n",
    "            return noisy_mag, clean_mag, noisy_mag.shape[-1], name\n",
    "        else:\n",
    "            return noisy, clean, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class WavDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Define train dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mixture_dataset,\n",
    "                 clean_dataset,\n",
    "                 limit=None,\n",
    "                 offset=0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Construct train dataset\n",
    "        Args:\n",
    "            mixture_dataset (str): mixture dir (wav format files)\n",
    "            clean_dataset (str): clean dir (wav format files)\n",
    "            limit (int): the limit of the dataset\n",
    "            offset (int): the offset of the dataset\n",
    "        \"\"\"\n",
    "        assert os.path.exists(mixture_dataset) and os.path.exists(clean_dataset)\n",
    "\n",
    "        print(\"Search datasets...\")\n",
    "        mixture_wav_files = librosa.util.find_files(mixture_dataset, ext=\"wav\", limit=limit, offset=offset)\n",
    "        clean_wav_files = librosa.util.find_files(clean_dataset, ext=\"wav\", limit=limit, offset=offset)\n",
    "\n",
    "        assert len(mixture_wav_files) == len(clean_wav_files)\n",
    "        print(f\"\\t Original length: {len(mixture_wav_files)}\")\n",
    "\n",
    "        self.length = len(mixture_wav_files)\n",
    "        self.mixture_wav_files = mixture_wav_files\n",
    "        self.clean_wav_files = clean_wav_files\n",
    "\n",
    "        print(f\"\\t Offset: {offset}\")\n",
    "        print(f\"\\t Limit: {limit}\")\n",
    "        print(f\"\\t Final length: {self.length}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        mixture_path = self.mixture_wav_files[item]\n",
    "        clean_path = self.clean_wav_files[item]\n",
    "        name = os.path.splitext(os.path.basename(clean_path))[0]\n",
    "\n",
    "        mixture, sr = sf.read(mixture_path, dtype=\"float32\")\n",
    "        clean, sr = sf.read(clean_path, dtype=\"float32\")\n",
    "        assert sr == 16000\n",
    "        assert mixture.shape == clean.shape\n",
    "\n",
    "        n_frames = (len(mixture) - 320) // 160 + 1\n",
    "\n",
    "        return mixture, clean, n_frames, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class WavDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Define train dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mixture_dataset,\n",
    "                 clean_dataset,\n",
    "                 limit=None,\n",
    "                 offset=0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Construct train dataset\n",
    "        Args:\n",
    "            mixture_dataset (str): mixture dir (wav format files)\n",
    "            clean_dataset (str): clean dir (wav format files)\n",
    "            limit (int): the limit of the dataset\n",
    "            offset (int): the offset of the dataset\n",
    "        \"\"\"\n",
    "        mixture_dataset = os.path.abspath(os.path.expanduser(mixture_dataset))\n",
    "        clean_dataset = os.path.abspath(os.path.expanduser(clean_dataset))\n",
    "        assert os.path.exists(mixture_dataset) and os.path.exists(clean_dataset)\n",
    "\n",
    "        print(\"Search datasets...\")\n",
    "        mixture_wav_files = librosa.util.find_files(mixture_dataset, ext=\"wav\", limit=limit, offset=offset)\n",
    "        clean_wav_files = librosa.util.find_files(clean_dataset, ext=\"wav\", limit=limit, offset=offset)\n",
    "\n",
    "        assert len(mixture_wav_files) == len(clean_wav_files)\n",
    "        print(f\"\\t Original length: {len(mixture_wav_files)}\")\n",
    "\n",
    "        self.length = len(mixture_wav_files)\n",
    "        self.mixture_wav_files = mixture_wav_files\n",
    "        self.clean_wav_files = clean_wav_files\n",
    "\n",
    "        print(f\"\\t Offset: {offset}\")\n",
    "        print(f\"\\t Limit: {limit}\")\n",
    "        print(f\"\\t Final length: {self.length}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        mixture_path = self.mixture_wav_files[item]\n",
    "        clean_path = self.clean_wav_files[item]\n",
    "        name = os.path.splitext(os.path.basename(clean_path))[0]\n",
    "\n",
    "        mixture, sr = sf.read(mixture_path, dtype=\"float32\")\n",
    "        clean, sr = sf.read(clean_path, dtype=\"float32\")\n",
    "        assert sr == 16000\n",
    "        assert mixture.shape == clean.shape\n",
    "\n",
    "        n_frames = (len(mixture) - 320) // 160 + 1\n",
    "\n",
    "        return mixture, clean, n_frames, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 161, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CausalConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 2),\n",
    "            stride=(2, 1),\n",
    "            padding=(0, 1)\n",
    "        )\n",
    "        self.norm = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.activation = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        2D Causal convolution.\n",
    "        Args:\n",
    "            x: [B, C, F, T]\n",
    "        Returns:\n",
    "            [B, C, F, T]\n",
    "        \"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = x[:, :, :, :-1]  # chomp size\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalTransConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, is_last=False, output_padding=(0, 0)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 2),\n",
    "            stride=(2, 1),\n",
    "            output_padding=output_padding\n",
    "        )\n",
    "        self.norm = nn.BatchNorm2d(num_features=out_channels)\n",
    "        if is_last:\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            self.activation = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        2D Causal convolution.\n",
    "        Args:\n",
    "            x: [B, C, F, T]\n",
    "        Returns:\n",
    "            [B, C, F, T]\n",
    "        \"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = x[:, :, :, :-1]  # chomp size\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CRN(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: [batch size, channels=1, T, n_fft]\n",
    "    Output: [batch size, T, n_fft]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CRN, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv_block_1 = CausalConvBlock(1, 16)\n",
    "        self.conv_block_2 = CausalConvBlock(16, 32)\n",
    "        self.conv_block_3 = CausalConvBlock(32, 64)\n",
    "        self.conv_block_4 = CausalConvBlock(64, 128)\n",
    "        self.conv_block_5 = CausalConvBlock(128, 256)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm_layer = nn.LSTM(input_size=1024, hidden_size=1024, num_layers=2, batch_first=True)\n",
    "\n",
    "        self.tran_conv_block_1 = CausalTransConvBlock(256 + 256, 128)\n",
    "        self.tran_conv_block_2 = CausalTransConvBlock(128 + 128, 64)\n",
    "        self.tran_conv_block_3 = CausalTransConvBlock(64 + 64, 32)\n",
    "        self.tran_conv_block_4 = CausalTransConvBlock(32 + 32, 16, output_padding=(1, 0))\n",
    "        self.tran_conv_block_5 = CausalTransConvBlock(16 + 16, 1, is_last=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm_layer.flatten_parameters()\n",
    "\n",
    "        e_1 = self.conv_block_1(x)\n",
    "        e_2 = self.conv_block_2(e_1)\n",
    "        e_3 = self.conv_block_3(e_2)\n",
    "        e_4 = self.conv_block_4(e_3)\n",
    "        e_5 = self.conv_block_5(e_4)  # [2, 256, 4, 200]\n",
    "\n",
    "        batch_size, n_channels, n_f_bins, n_frame_size = e_5.shape\n",
    "\n",
    "        # [2, 256, 4, 200] = [2, 1024, 200] => [2, 200, 1024]\n",
    "        lstm_in = e_5.reshape(batch_size, n_channels * n_f_bins, n_frame_size).permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm_layer(lstm_in)  # [2, 200, 1024]\n",
    "        lstm_out = lstm_out.permute(0, 2, 1).reshape(batch_size, n_channels, n_f_bins, n_frame_size)  # [2, 256, 4, 200]\n",
    "\n",
    "        d_1 = self.tran_conv_block_1(torch.cat((lstm_out, e_5), 1))\n",
    "        d_2 = self.tran_conv_block_2(torch.cat((d_1, e_4), 1))\n",
    "        d_3 = self.tran_conv_block_3(torch.cat((d_2, e_3), 1))\n",
    "        d_4 = self.tran_conv_block_4(torch.cat((d_3, e_2), 1))\n",
    "        d_5 = self.tran_conv_block_5(torch.cat((d_4, e_1), 1))\n",
    "\n",
    "        return d_5\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    layer = CRN()\n",
    "    a = torch.rand(2, 1, 161, 200)\n",
    "    print(layer(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def writer(logs_dir):\n",
    "    return SummaryWriter(log_dir=logs_dir, max_queue=5, flush_secs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: [batch size, channels=1, T, n_fft]\n",
    "    Output: [batch size, T, n_fft]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CRNN, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1, 3), stride=(1, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=16)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(1, 3), stride=(1, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=32)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1, 3), stride=(1, 2))\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=64)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1, 3), stride=(1, 2))\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=128)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(1, 3), stride=(1, 2))\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "\n",
    "        # LSTM\n",
    "        self.LSTM1 = nn.LSTM(input_size=1024, hidden_size=1024, num_layers=2, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.convT1 = nn.ConvTranspose2d(in_channels=512, out_channels=128, kernel_size=(1, 3), stride=(1, 2))\n",
    "        self.bnT1 = nn.BatchNorm2d(num_features=128)\n",
    "        self.convT2 = nn.ConvTranspose2d(in_channels=256, out_channels=64, kernel_size=(1, 3), stride=(1, 2))\n",
    "        self.bnT2 = nn.BatchNorm2d(num_features=64)\n",
    "        self.convT3 = nn.ConvTranspose2d(in_channels=128, out_channels=32, kernel_size=(1, 3), stride=(1, 2))\n",
    "        self.bnT3 = nn.BatchNorm2d(num_features=32)\n",
    "        # output_padding为1，不然算出来是79\n",
    "        self.convT4 = nn.ConvTranspose2d(in_channels=64, out_channels=16, kernel_size=(1, 3), stride=(1, 2), output_padding=(0, 1))\n",
    "        self.bnT4 = nn.BatchNorm2d(num_features=16)\n",
    "        self.convT5 = nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=(1, 3), stride=(1, 2))\n",
    "        self.bnT5 = nn.BatchNorm2d(num_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv\n",
    "        # (B, in_c, T, F)\n",
    "        x.unsqueeze_(1)\n",
    "        x1 = F.elu(self.bn1(self.conv1(x)))\n",
    "        x2 = F.elu(self.bn2(self.conv2(x1)))\n",
    "        x3 = F.elu(self.bn3(self.conv3(x2)))\n",
    "        x4 = F.elu(self.bn4(self.conv4(x3)))\n",
    "        x5 = F.elu(self.bn5(self.conv5(x4)))\n",
    "        # reshape\n",
    "        out5 = x5.permute(0, 2, 1, 3)\n",
    "        out5 = out5.reshape(out5.size()[0], out5.size()[1], -1)\n",
    "        # lstm\n",
    "\n",
    "        lstm, (hn, cn) = self.LSTM1(out5)\n",
    "        # reshape\n",
    "        output = lstm.reshape(lstm.size()[0], lstm.size()[1], 256, -1)\n",
    "        output = output.permute(0, 2, 1, 3)\n",
    "        # ConvTrans\n",
    "        res = torch.cat((output, x5), 1)\n",
    "        res1 = F.elu(self.bnT1(self.convT1(res)))\n",
    "        res1 = torch.cat((res1, x4), 1)\n",
    "        res2 = F.elu(self.bnT2(self.convT2(res1)))\n",
    "        res2 = torch.cat((res2, x3), 1)\n",
    "        res3 = F.elu(self.bnT3(self.convT3(res2)))\n",
    "        res3 = torch.cat((res3, x2), 1)\n",
    "        res4 = F.elu(self.bnT4(self.convT4(res3)))\n",
    "        res4 = torch.cat((res4, x1), 1)\n",
    "        # (B, o_c, T. F)\n",
    "        res5 = F.relu(self.bnT5(self.convT5(res4)))\n",
    "        return res5.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def mse_loss_for_variable_length_data():\n",
    "    def loss_function(target, ipt, n_frames_list, device):\n",
    "        \"\"\"\n",
    "        Calculate the MSE loss for variable length dataset.\n",
    "        ipt: [B, F, T]\n",
    "        target: [B, F, T]\n",
    "        \"\"\"\n",
    "        if target.shape[0] == 1:\n",
    "            return torch.nn.functional.mse_loss(target, ipt)\n",
    "\n",
    "        E = 1e-8\n",
    "        with torch.no_grad():\n",
    "            masks = []\n",
    "            for n_frames in n_frames_list:\n",
    "                masks.append(torch.ones(n_frames, target.size(1), dtype=torch.float32))  # the shape is (T_real, F)\n",
    "\n",
    "            binary_mask = pad_sequence(masks, batch_first=True).to(device).permute(0, 2, 1)  # ([T1, F], [T2, F]) => [B, T, F] => [B, F, T]\n",
    "\n",
    "        masked_ipt = ipt * binary_mask  # [B, F, T]\n",
    "        masked_target = target * binary_mask\n",
    "        return ((masked_ipt - masked_target) ** 2).sum() / (binary_mask.sum() + E)  # 不算 pad 部分的贡献，仅计算有效值\n",
    "\n",
    "    return loss_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pesq import pesq\n",
    "from pystoi.stoi import stoi\n",
    "\n",
    "\n",
    "def SI_SDR(reference, estimation):\n",
    "    \"\"\"\n",
    "    Scale-Invariant Signal-to-Distortion Ratio (SI-SDR)\n",
    "    Args:\n",
    "        reference: numpy.ndarray, [..., T]\n",
    "        estimation: numpy.ndarray, [..., T]\n",
    "    Returns:\n",
    "        SI-SDR\n",
    "    [1] SDR– Half- Baked or Well Done?\n",
    "    http://www.merl.com/publications/docs/TR2019-013.pdf\n",
    "    \"\"\"\n",
    "    estimation, reference = np.broadcast_arrays(estimation, reference)\n",
    "    reference_energy = np.sum(reference ** 2, axis=-1, keepdims=True)\n",
    "\n",
    "    # This is $\\alpha$ after Equation (3) in [1].\n",
    "    optimal_scaling = np.sum(reference * estimation, axis=-1, keepdims=True) \\\n",
    "                      / reference_energy\n",
    "\n",
    "    # This is $e_{\\text{target}}$ in Equation (4) in [1].\n",
    "    projection = optimal_scaling * reference\n",
    "\n",
    "    # This is $e_{\\text{res}}$ in Equation (4) in [1].\n",
    "    noise = estimation - projection\n",
    "\n",
    "    ratio = np.sum(projection ** 2, axis=-1) / np.sum(noise ** 2, axis=-1)\n",
    "    return 10 * np.log10(ratio)\n",
    "\n",
    "def STOI(ref, est, sr=16000):\n",
    "    return stoi(ref, est, sr, extended=False)\n",
    "\n",
    "\n",
    "def PESQ(ref, est, sr=16000):\n",
    "    return pesq(sr, ref, est, \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pesq import pesq\n",
    "from pystoi.stoi import stoi\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, device):\n",
    "    _, ext = os.path.splitext(os.path.basename(checkpoint_path))\n",
    "    assert ext in (\".pth\", \".tar\"), \"Only support ext and tar extensions of model checkpoint.\"\n",
    "    model_checkpoint = torch.load(os.path.abspath(os.path.expanduser(checkpoint_path)), map_location=device)\n",
    "\n",
    "    if ext == \".pth\":\n",
    "        print(f\"Loading {checkpoint_path}.\")\n",
    "        return model_checkpoint\n",
    "    else:  # tar\n",
    "        print(f\"Loading {checkpoint_path}, epoch = {model_checkpoint['epoch']}.\")\n",
    "        return model_checkpoint[\"model\"]\n",
    "\n",
    "\n",
    "def get_sub_band_bound(idx, n_bins, n_neighbor):\n",
    "    \"\"\"\n",
    "    根据索引来获取上下界限\n",
    "    Args:\n",
    "        idx: 当前索引\n",
    "        n_bins: 总共的频带数量\n",
    "        n_neighbor: 每侧拓展的频率带数量\n",
    "    Returns:\n",
    "        (子带上界的索引，子带下界的索引)\n",
    "    \"\"\"\n",
    "    # 随机取子带区间\n",
    "    n_bins_bottom = np.min([(n_bins - 1) - idx, n_neighbor])\n",
    "    n_bins_top = np.min([idx, n_neighbor])\n",
    "\n",
    "    # 补齐上边或者下边的长度\n",
    "    if n_bins_bottom < n_neighbor:\n",
    "        n_bins_top += n_neighbor - n_bins_bottom\n",
    "    elif n_bins_top < n_neighbor:\n",
    "        n_bins_bottom += n_neighbor - n_bins_top\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    idx_bottom_bound = idx + n_bins_bottom\n",
    "    idx_top_bound = idx - n_bins_top\n",
    "    return idx_top_bound, idx_bottom_bound\n",
    "\n",
    "\n",
    "def overlap_cat(chunk_list, dim=-1):\n",
    "    \"\"\"\n",
    "    按照 50% 的 overlap 沿着最后一个维度对 chunk_list 进行拼接\n",
    "    Args:\n",
    "        dim: 需要拼接的维度\n",
    "        chunk_list(list): [[B, T], [B, T], ...]\n",
    "    Returns:\n",
    "        overlap 拼接后\n",
    "    \"\"\"\n",
    "    overlap_output = []\n",
    "    for i, chunk in enumerate(chunk_list):\n",
    "        first_half, last_half = torch.split(chunk, chunk.size(-1) // 2, dim=dim)\n",
    "        if i == 0:\n",
    "            overlap_output += [first_half, last_half]\n",
    "        else:\n",
    "            overlap_output[-1] = (overlap_output[-1] + first_half) / 2\n",
    "            overlap_output.append(last_half)\n",
    "\n",
    "    overlap_output = torch.cat(overlap_output, dim=dim)\n",
    "    return overlap_output\n",
    "\n",
    "\n",
    "def prepare_empty_dir(dirs, resume=False):\n",
    "    \"\"\"\n",
    "    if resume experiment, assert the dirs exist,\n",
    "    if not resume experiment, make dirs.\n",
    "    Args:\n",
    "        dirs (list): directors list\n",
    "        resume (bool): whether to resume experiment, default is False\n",
    "    \"\"\"\n",
    "    for dir_path in dirs:\n",
    "        if resume:\n",
    "            assert dir_path.exists()\n",
    "        else:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "class ExecutionTime:\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "        timer = ExecutionTime()\n",
    "        <Something...>\n",
    "        print(f'Finished in {timer.duration()} seconds.')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def duration(self):\n",
    "        return int(time.time() - self.start_time)\n",
    "\n",
    "\n",
    "def initialize_config(module_cfg, pass_args=True):\n",
    "    \"\"\"\n",
    "    According to config items, load specific module dynamically with params.\n",
    "    eg，config items as follow：\n",
    "        module_cfg = {\n",
    "            \"module\": \"model.model\",\n",
    "            \"main\": \"Model\",\n",
    "            \"args\": {...}\n",
    "        }\n",
    "    1. Load the module corresponding to the \"module\" param.\n",
    "    2. Call function (or instantiate class) corresponding to the \"main\" param.\n",
    "    3. Send the param (in \"args\") into the function (or class) when calling ( or instantiating)\n",
    "    \"\"\"\n",
    "    module = importlib.import_module(module_cfg[\"module\"])\n",
    "\n",
    "    if pass_args:\n",
    "        return getattr(module, module_cfg[\"main\"])(**module_cfg[\"args\"])\n",
    "    else:\n",
    "        return getattr(module, module_cfg[\"main\"])\n",
    "\n",
    "\n",
    "def compute_PESQ(clean_signal, noisy_signal, sr=16000):\n",
    "    return pesq(sr, clean_signal, noisy_signal, \"wb\")\n",
    "\n",
    "\n",
    "def z_score(m):\n",
    "    mean = np.mean(m)\n",
    "    std_var = np.std(m)\n",
    "    return (m - mean) / std_var, mean, std_var\n",
    "\n",
    "\n",
    "def reverse_z_score(m, mean, std_var):\n",
    "    return m * std_var + mean\n",
    "\n",
    "\n",
    "def min_max(m):\n",
    "    m_max = np.max(m)\n",
    "    m_min = np.min(m)\n",
    "\n",
    "    return (m - m_min) / (m_max - m_min), m_max, m_min\n",
    "\n",
    "\n",
    "def reverse_min_max(m, m_max, m_min):\n",
    "    return m * (m_max - m_min) + m_min\n",
    "\n",
    "\n",
    "def sample_fixed_length_data_aligned(data_a, data_b, sample_length):\n",
    "    \"\"\"\n",
    "    从某个随机位置开始，从两个样本中取出固定长度的片段\n",
    "    \"\"\"\n",
    "    assert data_a.shape == data_b.shape, \"Inconsistent dataset size.\"\n",
    "    dim = np.ndim(data_a)\n",
    "    assert dim == 1 or dim == 2, \"Only support 1D or 2D.\"\n",
    "\n",
    "    if data_a.shape[-1] > sample_length:\n",
    "        frames_total = data_a.shape[-1]\n",
    "        start = np.random.randint(frames_total - sample_length + 1)\n",
    "        end = start + sample_length\n",
    "        if dim == 1:\n",
    "            return data_a[start:end], data_b[start:end]\n",
    "        else:\n",
    "            return data_a[:, start:end], data_b[:, start:end]\n",
    "    elif data_a.shape[-1] == sample_length:\n",
    "        return data_a, data_b\n",
    "    else:\n",
    "        frames_total = data_a.shape[-1]\n",
    "        if dim == 1:\n",
    "            return np.append(\n",
    "                data_a,\n",
    "                np.zeros(sample_length - frames_total, dtype=np.float32)\n",
    "            ), np.append(\n",
    "                data_b,\n",
    "                np.zeros(sample_length - frames_total, dtype=np.float32)\n",
    "            )\n",
    "        else:\n",
    "            return np.append(\n",
    "                data_a,\n",
    "                np.zeros(shape=(data_a.shape[0], sample_length - frames_total), dtype=np.float32),\n",
    "                axis=-1\n",
    "            ), np.append(\n",
    "                data_b,\n",
    "                np.zeros(shape=(data_a.shape[0], sample_length - frames_total), dtype=np.float32),\n",
    "                axis=-1\n",
    "            )\n",
    "\n",
    "\n",
    "def compute_STOI(clean_signal, noisy_signal, sr=16000):\n",
    "    return stoi(clean_signal, noisy_signal, sr, extended=False)\n",
    "\n",
    "\n",
    "def print_tensor_info(tensor, flag=\"Tensor\"):\n",
    "    floor_tensor = lambda float_tensor: int(float(float_tensor) * 1000) / 1000\n",
    "    print(flag)\n",
    "    print(\n",
    "        f\"\\tmax: {floor_tensor(torch.max(tensor))}, min: {float(torch.min(tensor))}, mean: {floor_tensor(torch.mean(tensor))}, std: {floor_tensor(torch.std(tensor))}\")\n",
    "\n",
    "\n",
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        nets: list of networks\n",
    "        requires_grad\n",
    "    \"\"\"\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        if net is not None:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "\n",
    "def prepare_device(n_gpu: int, cudnn_deterministic=False):\n",
    "    \"\"\"Choose to use CPU or GPU depend on \"n_gpu\".\n",
    "    Args:\n",
    "        n_gpu(int): the number of GPUs used in the experiment.\n",
    "            if n_gpu is 0, use CPU;\n",
    "            if n_gpu > 1, use GPU.\n",
    "        cudnn_deterministic (bool): repeatability\n",
    "            cudnn.benchmark will find algorithms to optimize training. if we need to consider the repeatability of experiment, set use_cudnn_deterministic to True\n",
    "    \"\"\"\n",
    "    if n_gpu == 0:\n",
    "        print(\"Using CPU in the experiment.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        if cudnn_deterministic:\n",
    "            print(\"Using CuDNN deterministic mode in the experiment.\")\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        device = torch.device(\"cuda:0\")\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import json5\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from util import visualization\n",
    "from util.metrics import STOI, PESQ, SI_SDR\n",
    "from util.utils import prepare_empty_dir, ExecutionTime, prepare_device\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "class BaseTrainer:\n",
    "    def __init__(self, config, resume: bool, model, loss_function, optimizer):\n",
    "        self.n_gpu = torch.cuda.device_count()\n",
    "        self.device = prepare_device(self.n_gpu, cudnn_deterministic=config[\"cudnn_deterministic\"])\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        if self.n_gpu > 1:\n",
    "            self.model = torch.nn.DataParallel(self.model, device_ids=list(range(self.n_gpu)))\n",
    "\n",
    "        # Trainer\n",
    "        self.epochs = config[\"trainer\"][\"epochs\"]\n",
    "        self.save_checkpoint_interval = config[\"trainer\"][\"save_checkpoint_interval\"]\n",
    "        self.validation_config = config[\"trainer\"][\"validation\"]\n",
    "        self.train_config = config[\"trainer\"].get(\"train\", {})\n",
    "        self.validation_interval = self.validation_config[\"interval\"]\n",
    "        self.find_max = self.validation_config[\"find_max\"]\n",
    "        self.validation_custom_config = self.validation_config[\"custom\"]\n",
    "        self.train_custom_config = self.train_config.get(\"custom\", {})\n",
    "\n",
    "        # The following args is not in the config file, We will update it if resume is True in later.\n",
    "        self.start_epoch = 1\n",
    "        self.best_score = -np.inf if self.find_max else np.inf\n",
    "        self.root_dir = Path(config[\"root_dir\"]).expanduser().absolute() / config[\"experiment_name\"]\n",
    "        self.checkpoints_dir = self.root_dir / \"checkpoints\"\n",
    "        self.logs_dir = self.root_dir / \"logs\"\n",
    "        prepare_empty_dir([self.checkpoints_dir, self.logs_dir], resume=resume)\n",
    "\n",
    "        self.writer = visualization.writer(self.logs_dir.as_posix())\n",
    "        self.writer.add_text(\n",
    "            tag=\"Configuration\",\n",
    "            text_string=f\"<pre>  \\n{json5.dumps(config, indent=4, sort_keys=False)}  \\n</pre>\",\n",
    "            global_step=1\n",
    "        )\n",
    "\n",
    "        if resume: self._resume_checkpoint()\n",
    "        if config[\"preloaded_model_path\"]: self._preload_model(Path(config[\"preloaded_model_path\"]))\n",
    "\n",
    "        print(\"Configurations are as follows: \")\n",
    "        print(json5.dumps(config, indent=2, sort_keys=False))\n",
    "\n",
    "        with open((self.root_dir / f\"{time.strftime('%Y-%m-%d-%H-%M-%S')}.json\").as_posix(), \"w\") as handle:\n",
    "            json5.dump(config, handle, indent=2, sort_keys=False)\n",
    "\n",
    "        self._print_networks([self.model])\n",
    "\n",
    "    def _preload_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Preload *.pth file of the model at the start of the current experiment.\n",
    "        Args:\n",
    "            model_path(Path): the path of the *.pth file\n",
    "        \"\"\"\n",
    "        model_path = model_path.expanduser().absolute()\n",
    "        assert model_path.exists(), f\"Preloaded *.pth file is not exist. Please check the file path: {model_path.as_posix()}\"\n",
    "        model_checkpoint = torch.load(model_path.as_posix(), map_location=self.device)\n",
    "\n",
    "        if isinstance(self.model, torch.nn.DataParallel):\n",
    "            self.model.module.load_state_dict(model_checkpoint, strict=False)\n",
    "        else:\n",
    "            self.model.load_state_dict(model_checkpoint, strict=False)\n",
    "\n",
    "        print(f\"Model preloaded successfully from {model_path.as_posix()}.\")\n",
    "\n",
    "    def _resume_checkpoint(self):\n",
    "        \"\"\"Resume experiment from latest checkpoint.\n",
    "        Notes:\n",
    "            To be careful at Loading model. if model is an instance of DataParallel, we need to set model.module.*\n",
    "        \"\"\"\n",
    "        latest_model_path = self.checkpoints_dir.expanduser().absolute() / \"latest_model.tar\"\n",
    "        assert latest_model_path.exists(), f\"{latest_model_path} does not exist, can not load latest checkpoint.\"\n",
    "\n",
    "        checkpoint = torch.load(latest_model_path.as_posix(), map_location=self.device)\n",
    "\n",
    "        self.start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        self.best_score = checkpoint[\"best_score\"]\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        if isinstance(self.model, torch.nn.DataParallel):\n",
    "            self.model.module.load_state_dict(checkpoint[\"model\"])\n",
    "        else:\n",
    "            self.model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "        print(f\"Model checkpoint loaded. Training will begin in {self.start_epoch} epoch.\")\n",
    "\n",
    "    def _save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save checkpoint to <root_dir>/checkpoints directory, which contains:\n",
    "            - current epoch\n",
    "            - best score in history\n",
    "            - optimizer parameters\n",
    "            - model parameters\n",
    "        Args:\n",
    "            is_best(bool): if current checkpoint got the best score, it also will be saved in <root_dir>/checkpoints/best_model.tar.\n",
    "        \"\"\"\n",
    "        print(f\"\\t Saving {epoch} epoch model checkpoint...\")\n",
    "\n",
    "        # Construct checkpoint tar package\n",
    "        state_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"best_score\": self.best_score,\n",
    "            \"optimizer\": self.optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        if isinstance(self.model, torch.nn.DataParallel):  # Parallel\n",
    "            state_dict[\"model\"] = self.model.module.cpu().state_dict()\n",
    "        else:\n",
    "            state_dict[\"model\"] = self.model.cpu().state_dict()\n",
    "\n",
    "        \"\"\"\n",
    "        Notes:\n",
    "            - latest_model.tar:\n",
    "                Contains all checkpoint information, including optimizer parameters, model parameters, etc. New checkpoint will overwrite old one.\n",
    "            - model_<epoch>.pth: \n",
    "                The parameters of the model. Follow-up we can specify epoch to inference.\n",
    "            - best_model.tar:\n",
    "                Like latest_model, but only saved when <is_best> is True.\n",
    "        \"\"\"\n",
    "        torch.save(state_dict, (self.checkpoints_dir / \"latest_model.tar\").as_posix())\n",
    "        torch.save(state_dict[\"model\"], (self.checkpoints_dir / f\"model_{str(epoch).zfill(4)}.pth\").as_posix())\n",
    "        if is_best:\n",
    "            print(f\"\\t Found best score in {epoch} epoch, saving...\")\n",
    "            torch.save(state_dict, (self.checkpoints_dir / \"best_model.tar\").as_posix())\n",
    "\n",
    "        # Use model.cpu() or model.to(\"cpu\") will migrate the model to CPU, at which point we need re-migrate model back.\n",
    "        # No matter tensor.cuda() or tensor.to(\"cuda\"), if tensor in CPU, the tensor will not be migrated to GPU, but the model will.\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _is_best(self, score, find_max=True):\n",
    "        \"\"\"Check if the current model is the best model\n",
    "        \"\"\"\n",
    "        if find_max and score >= self.best_score:\n",
    "            self.best_score = score\n",
    "            return True\n",
    "        elif not find_max and score <= self.best_score:\n",
    "            self.best_score = score\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def _transform_pesq_range(pesq_score):\n",
    "        \"\"\"transform [-0.5 ~ 4.5] to [0 ~ 1]\n",
    "        \"\"\"\n",
    "        return (pesq_score + 0.5) / 5\n",
    "\n",
    "    @staticmethod\n",
    "    def _print_networks(nets: list):\n",
    "        print(f\"This project contains {len(nets)} networks, the number of the parameters: \")\n",
    "        params_of_all_networks = 0\n",
    "        for i, net in enumerate(nets, start=1):\n",
    "            params_of_network = 0\n",
    "            for param in net.parameters():\n",
    "                params_of_network += param.numel()\n",
    "\n",
    "            print(f\"\\tNetwork {i}: {params_of_network / 1e6} million.\")\n",
    "            params_of_all_networks += params_of_network\n",
    "\n",
    "        print(f\"The amount of parameters in the project is {params_of_all_networks / 1e6} million.\")\n",
    "\n",
    "    def _set_models_to_train_mode(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def _set_models_to_eval_mode(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def spec_audio_visualization(self, noisy, enhanced, clean, name, epoch):\n",
    "        # Visualize audio\n",
    "        self.writer.add_audio(f\"Speech/{name}_Noisy\", noisy, epoch, sample_rate=16000)\n",
    "        self.writer.add_audio(f\"Speech/{name}_Enhanced\", enhanced, epoch, sample_rate=16000)\n",
    "        self.writer.add_audio(f\"Speech/{name}_Clean\", clean, epoch, sample_rate=16000)\n",
    "\n",
    "        # # Visualize waveform\n",
    "        # fig, ax = plt.subplots(3, 1)\n",
    "        # for j, y in enumerate([noisy, enhanced, clean]):\n",
    "        #     ax[j].set_title(\"mean: {:.3f}, std: {:.3f}, max: {:.3f}, min: {:.3f}\".format(\n",
    "        #         np.mean(y),\n",
    "        #         np.std(y),\n",
    "        #         np.max(y),\n",
    "        #         np.min(y)\n",
    "        #     ))\n",
    "        #     librosa.display.waveplot(y, sr=16000, ax=ax[j])\n",
    "        # plt.tight_layout()\n",
    "        # self.writer.add_figure(f\"Waveform/{name}\", fig, epoch)\n",
    "\n",
    "        # Visualize spectrogram\n",
    "        noisy_mag, _ = librosa.magphase(librosa.stft(noisy, n_fft=320, hop_length=160, win_length=320))\n",
    "        enhanced_mag, _ = librosa.magphase(librosa.stft(enhanced, n_fft=320, hop_length=160, win_length=320))\n",
    "        clean_mag, _ = librosa.magphase(librosa.stft(clean, n_fft=320, hop_length=160, win_length=320))\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(6, 6))\n",
    "        for k, mag in enumerate([\n",
    "            noisy_mag,\n",
    "            enhanced_mag,\n",
    "            clean_mag,\n",
    "        ]):\n",
    "            axes[k].set_title(f\"mean: {np.mean(mag):.3f}, \"\n",
    "                              f\"std: {np.std(mag):.3f}, \"\n",
    "                              f\"max: {np.max(mag):.3f}, \"\n",
    "                              f\"min: {np.min(mag):.3f}\")\n",
    "            librosa.display.specshow(librosa.amplitude_to_db(mag), cmap=\"magma\", y_axis=\"linear\", ax=axes[k], sr=16000)\n",
    "        plt.tight_layout()\n",
    "        self.writer.add_figure(f\"Spectrogram/{name}\", fig, epoch)\n",
    "\n",
    "    def metrics_visualization(self, noisy_list, clean_list, enhanced_list, epoch):\n",
    "        stoi_clean_noisy = []  # Clean and noisy\n",
    "        stoi_clean_denoise = []  # Clean and denoisy\n",
    "\n",
    "        pesq_clean_noisy = []\n",
    "        pesq_clean_denoise = []\n",
    "\n",
    "        sisdr_clean_noisy = []\n",
    "        sisdr_clean_denoise = []\n",
    "\n",
    "        for noisy, enhanced, clean in zip(noisy_list, enhanced_list, clean_list):\n",
    "            stoi_clean_noisy.append(STOI(clean, noisy, sr=16000))\n",
    "            stoi_clean_denoise.append(STOI(clean, enhanced, sr=16000))\n",
    "\n",
    "            pesq_clean_noisy.append(PESQ(clean, noisy, sr=16000))\n",
    "            pesq_clean_denoise.append(PESQ(clean, enhanced, sr=16000))\n",
    "\n",
    "            sisdr_clean_noisy.append(SI_SDR(clean, noisy))\n",
    "            sisdr_clean_denoise.append(SI_SDR(clean, enhanced))\n",
    "\n",
    "        self.writer.add_scalars(f\"Validation/STOI\", {\n",
    "            \"clean and noisy\": np.mean(stoi_clean_noisy),\n",
    "            \"clean and enhanced\": np.mean(stoi_clean_denoise)\n",
    "        }, epoch)\n",
    "        self.writer.add_scalars(f\"Validation/PESQ\", {\n",
    "            \"clean and noisy\": np.mean(pesq_clean_noisy),\n",
    "            \"clean and enhanced\": np.mean(pesq_clean_denoise)\n",
    "        }, epoch)\n",
    "        self.writer.add_scalars(f\"Validation/SI-SDR\", {\n",
    "            \"clean and noisy\": np.mean(sisdr_clean_noisy),\n",
    "            \"clean and enhanced\": np.mean(sisdr_clean_denoise)\n",
    "        }, epoch)\n",
    "\n",
    "        return (self._transform_pesq_range(np.mean(pesq_clean_denoise)) + np.mean(stoi_clean_denoise)) / 2\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            print(f\"============== {epoch} epoch ==============\")\n",
    "            print(\"[0 seconds] Begin training...\")\n",
    "            timer = ExecutionTime()\n",
    "\n",
    "            self._set_models_to_train_mode()\n",
    "            self._train_epoch(epoch)\n",
    "\n",
    "            if self.save_checkpoint_interval != 0 and (epoch % self.save_checkpoint_interval == 0):\n",
    "                self._save_checkpoint(epoch)\n",
    "\n",
    "            if self.validation_interval != 0 and epoch % self.validation_interval == 0:\n",
    "                print(f\"[{timer.duration()} seconds] Training is over, Validation is in progress...\")\n",
    "\n",
    "                self._set_models_to_eval_mode()\n",
    "                score = self._validation_epoch(epoch)\n",
    "\n",
    "                if self._is_best(score, find_max=self.find_max):\n",
    "                    self._save_checkpoint(epoch, is_best=True)\n",
    "\n",
    "            print(f\"[{timer.duration()} seconds] End this epoch.\")\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _validation_epoch(self, epoch):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trainer.base_trainer import BaseTrainer\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "class Trainer(BaseTrainer):\n",
    "    def __init__(self, config, resume: bool, model, loss_function, optimizer, train_dataloader, validation_dataloader):\n",
    "        super(Trainer, self).__init__(config, resume, model, loss_function, optimizer)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.validation_dataloader = validation_dataloader\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        loss_total = 0.0\n",
    "\n",
    "        for noisy, clean, n_frames_list, _ in self.train_dataloader:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            noisy = noisy.to(self.device).unsqueeze(1)  # [B, F, T] => [B, 1, F, T]\n",
    "            clean = clean.to(self.device).unsqueeze(1)  # [B, F, T] => [B, 1, F, T]\n",
    "\n",
    "            enhanced = self.model(noisy)  # [B, 1, F, T]\n",
    "\n",
    "            loss = self.loss_function(enhanced, clean, n_frames_list, self.device)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.item()\n",
    "\n",
    "        self.writer.add_scalar(f\"Loss/Train\", loss_total / len(self.train_dataloader), epoch)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _validation_epoch(self, epoch):\n",
    "        noisy_list = []\n",
    "        clean_list = []\n",
    "        enhanced_list = []\n",
    "\n",
    "        loss_total = 0.0\n",
    "\n",
    "        visualization_limit = self.validation_custom_config[\"visualization_limit\"]\n",
    "        n_fft = self.validation_custom_config[\"n_fft\"]\n",
    "        hop_length = self.validation_custom_config[\"hop_length\"]\n",
    "        win_length = self.validation_custom_config[\"win_length\"]\n",
    "\n",
    "        for i, (noisy, clean, name) in tqdm(enumerate(self.validation_dataloader), desc=\"Inference\"):\n",
    "            assert len(name) == 1, \"The batch size of inference stage must 1.\"\n",
    "            name = name[0]\n",
    "\n",
    "            noisy = noisy.numpy().reshape(-1)\n",
    "            clean = clean.numpy().reshape(-1)\n",
    "\n",
    "            noisy_mag, noisy_phase = librosa.magphase(librosa.stft(noisy, n_fft=n_fft, hop_length=hop_length, win_length=win_length))  # [T], [F, T]\n",
    "            clean_mag, _ = librosa.magphase(librosa.stft(clean, n_fft=n_fft, hop_length=hop_length, win_length=win_length))  # [T] => [F, T]\n",
    "\n",
    "            noisy_mag = torch.tensor(noisy_mag, device=self.device)[None, None, :, :]  # [F, T] => [1, 1, F, T]\n",
    "            clean_mag = torch.tensor(clean_mag, device=self.device)[None, None, :, :]\n",
    "\n",
    "            enhanced_mag = self.model(noisy_mag)\n",
    "\n",
    "            loss_total += self.loss_function(clean_mag, enhanced_mag, [clean_mag.shape[-1], ], self.device).item()\n",
    "\n",
    "            enhanced_mag = enhanced_mag.squeeze(0).squeeze(0).detach().cpu().numpy()  # [1, 1, F, T] => [F, T]\n",
    "            enhanced = librosa.istft(enhanced_mag * noisy_phase, hop_length=hop_length, win_length=win_length, length=len(noisy))\n",
    "\n",
    "            assert len(noisy) == len(clean) == len(enhanced)\n",
    "\n",
    "            if i <= np.min([visualization_limit, len(self.validation_dataloader)]):\n",
    "                \"\"\"======= 可视化第 i 个结果 =======\"\"\"\n",
    "                self.spec_audio_visualization(noisy, enhanced, clean, name, epoch)\n",
    "\n",
    "            noisy_list.append(noisy)\n",
    "            clean_list.append(clean)\n",
    "            enhanced_list.append(enhanced)\n",
    "\n",
    "        self.writer.add_scalar(f\"Loss/Validation\", loss_total / len(self.validation_dataloader), epoch)\n",
    "        return self.metrics_visualization(noisy_list, clean_list, enhanced_list, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from inferencer.inferencer import inference_wrapper\n",
    "from trainer.base_trainer import BaseTrainer\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "class Trainer(BaseTrainer):\n",
    "    def __init__(self, config, resume: bool, model, loss_function, optimizer, train_dataloader, validation_dataloader):\n",
    "        super(Trainer, self).__init__(config, resume, model, loss_function, optimizer)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.validation_dataloader = validation_dataloader\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        loss_total = 0.0\n",
    "\n",
    "        for noisy_mag, clean_mag, _ in self.train_dataloader:\n",
    "            noisy_mag = noisy_mag.to(self.device)\n",
    "            clean_mag = clean_mag.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            enhanced_mag = self.model(noisy_mag)\n",
    "\n",
    "            loss = self.loss_function(clean_mag, enhanced_mag)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            loss_total += loss.item()\n",
    "\n",
    "        self.writer.add_scalar(f\"Train/Loss\", loss_total / len(self.train_dataloader), epoch)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _validation_epoch(self, epoch):\n",
    "        noisy_list, enhanced_list, clean_list, name_list, loss = inference_wrapper(\n",
    "            dataloader=self.validation_dataloader,\n",
    "            model=self.model,\n",
    "            loss_function=self.loss_function,\n",
    "            device=self.device,\n",
    "            inference_args=self.validation_custom_config,\n",
    "            enhanced_dir=None\n",
    "        )\n",
    "\n",
    "        self.writer.add_scalar(f\"Validation/Loss\", loss, epoch)\n",
    "\n",
    "        for i in range(np.min([self.validation_custom_config[\"visualization_limit\"], len(self.validation_dataloader)])):\n",
    "            self.spec_audio_visualization(\n",
    "                noisy_list[i],\n",
    "                enhanced_list[i],\n",
    "                clean_list[i],\n",
    "                name_list[i],\n",
    "                epoch\n",
    "            )\n",
    "\n",
    "        score = self.metrics_visualization(noisy_list, clean_list, enhanced_list, epoch)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as functional\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "from trainer.base_trainer import BaseTrainer\n",
    "from util.utils import overlap_cat\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "class Trainer(BaseTrainer):\n",
    "    def __init__(self, config, resume: bool, model, loss_function, optimizer, train_dataloader, validation_dataloader):\n",
    "        super(Trainer, self).__init__(config, resume, model, loss_function, optimizer)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.validation_dataloader = validation_dataloader\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        loss_total = 0.0\n",
    "\n",
    "        for noisy, clean, _ in self.train_dataloader:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            noisy = noisy.to(self.device)  # [B, T]\n",
    "            clean = clean.to(self.device)  # [B, T]\n",
    "\n",
    "            noisy_d = torch.stft(\n",
    "                noisy,\n",
    "                n_fft=320,\n",
    "                hop_length=160,\n",
    "                win_length=320,\n",
    "                window=torch.hann_window(320).to(self.device))  # [B, F, T, 2]\n",
    "\n",
    "            noisy_mag, noisy_phase = torchaudio.functional.magphase(noisy_d)  # [B, F, T], [B, F, T]\n",
    "\n",
    "            enhanced_mag = self.model(noisy_mag)\n",
    "\n",
    "            enhanced_d = torch.cat([\n",
    "                (enhanced_mag * torch.cos(noisy_phase)).unsqueeze(-1),\n",
    "                (enhanced_mag * torch.sin(noisy_phase)).unsqueeze(-1)\n",
    "            ], dim=-1)  # [B, F, T, 2]\n",
    "\n",
    "            enhanced = torchaudio.functional.istft(\n",
    "                enhanced_d,\n",
    "                n_fft=320,\n",
    "                hop_length=160,\n",
    "                win_length=320,\n",
    "                window=torch.hann_window(320).to(self.device),\n",
    "                length=noisy.shape[1])\n",
    "\n",
    "            loss = self.loss_function(clean, enhanced)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.item()\n",
    "\n",
    "        self.writer.add_scalar(f\"Loss/Train\", loss_total / len(self.train_dataloader), epoch)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _validation_epoch(self, epoch):\n",
    "        noisy_list = []\n",
    "        clean_list = []\n",
    "        enhanced_list = []\n",
    "\n",
    "        loss_total = 0.0\n",
    "\n",
    "        visualization_limit = self.validation_custom_config[\"visualization_limit\"]\n",
    "        n_fft = self.validation_custom_config[\"n_fft\"]\n",
    "        hop_length = self.validation_custom_config[\"hop_length\"]\n",
    "        win_length = self.validation_custom_config[\"win_length\"]\n",
    "        batch_size = self.validation_custom_config[\"batch_size\"]\n",
    "        unfold_size = self.validation_custom_config[\"unfold_size\"]\n",
    "\n",
    "        for i, (noisy, clean, name) in tqdm(enumerate(self.validation_dataloader), desc=\"Inference\"):\n",
    "            assert len(name) == 1, \"The batch size of inference stage must 1.\"\n",
    "            name = name[0]\n",
    "            padded_length = 0  # 用于后续的 pad\n",
    "\n",
    "            noisy = noisy.to(self.device)  # [1, T]\n",
    "            clean = clean.to(self.device)  # [1, T]\n",
    "\n",
    "            noisy_d = torch.stft(\n",
    "                noisy,\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                win_length=win_length,\n",
    "                window=torch.hann_window(win_length).to(self.device))  # [B, F, T, 2]\n",
    "            noisy_mag, noisy_phase = torchaudio.functional.magphase(noisy_d)  # [1, F, T]\n",
    "\n",
    "            \"\"\"=== === === start overlap enhancement === === ===\"\"\"\n",
    "            noisy_mag = noisy_mag[None, :, :, :]  # [1, F, T] => [1, 1, F, T]，多一个维度是为了 unfold\n",
    "\n",
    "            if noisy_mag.size(-1) % unfold_size != 0:\n",
    "                padded_length = unfold_size - (noisy_mag.size(-1) % unfold_size)\n",
    "                noisy_mag = torch.cat([noisy_mag, torch.zeros(1, 1, noisy_mag.size(2), padded_length, device=self.device)], dim=-1)  # [1, 1, F, T]\n",
    "\n",
    "            noisy_mag = functional.unfold(noisy_mag, kernel_size=(n_fft // 2 + 1, unfold_size), stride=unfold_size // 2)\n",
    "            # [1, F, T, N] => [N, 1, F, T] => [N, F, T], where is #chunks.\n",
    "            noisy_mag = noisy_mag.reshape(1, n_fft // 2 + 1, unfold_size, -1).permute(3, 0, 1, 2).squeeze(1)\n",
    "            noisy_mag_chunks = torch.split(noisy_mag, batch_size, dim=0)  # [N, F, T] => ([B, F, T], ...), where the number is N // batch_size + 1\n",
    "\n",
    "            enhanced_mag_chunks = []\n",
    "            for noisy_mag_chunk in noisy_mag_chunks:\n",
    "                enhanced_mag_chunk = self.model(noisy_mag_chunk)  # [1, F, T]\n",
    "                enhanced_mag_chunks += torch.split(enhanced_mag_chunk, 1, dim=0)  # [B, F, T] => ([1, F, T], [1, F, T], ...)\n",
    "\n",
    "            enhanced_mag = overlap_cat(enhanced_mag_chunks)  # ([1, F, T], [1, F, T], ...) => [1, F, T]\n",
    "            enhanced_mag = enhanced_mag if padded_length == 0 else enhanced_mag[:, :, :-padded_length]  # [1, F, T]\n",
    "            \"\"\"=== === === end overlap enhancement === === ===\"\"\"\n",
    "\n",
    "            enhanced_d = torch.cat([\n",
    "                (enhanced_mag * torch.cos(noisy_phase)).unsqueeze(-1),\n",
    "                (enhanced_mag * torch.sin(noisy_phase)).unsqueeze(-1)\n",
    "            ], dim=-1)  # [B, F, T, 2]\n",
    "\n",
    "            enhanced = torchaudio.functional.istft(\n",
    "                enhanced_d,\n",
    "                n_fft=n_fft,\n",
    "                hop_length=hop_length,\n",
    "                win_length=win_length,\n",
    "                window=torch.hann_window(win_length).to(self.device),\n",
    "                length=noisy.shape[1])  # [1, T]\n",
    "\n",
    "            loss_total += self.loss_function(clean, enhanced).item()\n",
    "\n",
    "            noisy = noisy.detach().squeeze(0).cpu().numpy()\n",
    "            clean = clean.detach().squeeze(0).cpu().numpy()\n",
    "            enhanced = enhanced.detach().squeeze(0).cpu().numpy()\n",
    "\n",
    "            assert len(noisy) == len(clean) == len(enhanced)\n",
    "\n",
    "            if i <= np.min([visualization_limit, len(self.validation_dataloader)]):\n",
    "                \"\"\"======= 可视化第 i 个结果 =======\"\"\"\n",
    "                self.spec_audio_visualization(noisy, enhanced, clean, name, epoch)\n",
    "\n",
    "            noisy_list.append(noisy)\n",
    "            clean_list.append(clean)\n",
    "            enhanced_list.append(enhanced)\n",
    "\n",
    "        self.writer.add_scalar(f\"Loss/Validation\", loss_total / len(self.validation_dataloader), epoch)\n",
    "        return self.metrics_visualization(noisy_list, clean_list, enhanced_list, epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
